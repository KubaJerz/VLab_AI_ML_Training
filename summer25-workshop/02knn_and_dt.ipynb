plt.tight_layout()
plt.show()

# Select best models and compare final performance
best_dt = DecisionTreeClassifier(max_depth=8, random_state=42)  # Good balance
best_rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Standard choice

best_dt.fit(X_train_student, y_train_student)
best_rf.fit(X_train_student, y_train_student)

dt_pred_final = best_dt.predict(X_test_student)
rf_pred_final = best_rf.predict(X_test_student)

print("\nüèÜ Final Model Comparison: Student Success Prediction")
print("=" * 60)
print(f"Decision Tree (depth=8):  {accuracy_score(y_test_student, dt_pred_final):.3f}")
print(f"Random Forest (100 trees): {accuracy_score(y_test_student, rf_pred_final):.3f}")

# Detailed analysis
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Confusion matrices
cm_dt_final = confusion_matrix(y_test_student, dt_pred_final)
cm_rf_final = confusion_matrix(y_test_student, rf_pred_final)

disp_dt_final = ConfusionMatrixDisplay(confusion_matrix=cm_dt_final, 
                                      display_labels=['Did Not Graduate', 'Graduated'])
disp_dt_final.plot(ax=axes[0,0], cmap='Blues')
axes[0,0].set_title('Decision Tree\nConfusion Matrix')

disp_rf_final = ConfusionMatrixDisplay(confusion_matrix=cm_rf_final, 
                                      display_labels=['Did Not Graduate', 'Graduated'])
disp_rf_final.plot(ax=axes[0,1], cmap='Greens')
axes[0,1].set_title('Random Forest\nConfusion Matrix')

# Feature importance
dt_importance = best_dt.feature_importances_
rf_importance = best_rf.feature_importances_

x_pos = np.arange(len(features_student))
width = 0.35

axes[0,2].bar(x_pos - width/2, dt_importance, width, label='Decision Tree', alpha=0.7)
axes[0,2].bar(x_pos + width/2, rf_importance, width, label='Random Forest', alpha=0.7)
axes[0,2].set_xlabel('Features')
axes[0,2].set_ylabel('Importance')
axes[0,2].set_title('Feature Importance Comparison')
axes[0,2].set_xticks(x_pos)
axes[0,2].set_xticklabels(features_student, rotation=45)
axes[0,2].legend()

# Model interpretation: Show decision tree
plot_tree(best_dt, 
         feature_names=features_student,
         class_names=['Did Not Graduate', 'Graduated'],
         filled=True,
         rounded=True,
         fontsize=8,
         ax=axes[1,0])
axes[1,0].set_title('Decision Tree Structure')

# Performance metrics
from sklearn.metrics import precision_score, recall_score, f1_score

metrics_data = {
    'Model': ['Decision Tree', 'Random Forest'],
    'Accuracy': [accuracy_score(y_test_student, dt_pred_final), 
                accuracy_score(y_test_student, rf_pred_final)],
    'Precision': [precision_score(y_test_student, dt_pred_final), 
                 precision_score(y_test_student, rf_pred_final)],
    'Recall': [recall_score(y_test_student, dt_pred_final), 
              recall_score(y_test_student, rf_pred_final)],
    'F1-Score': [f1_score(y_test_student, dt_pred_final), 
                f1_score(y_test_student, rf_pred_final)]
}

metrics_df = pd.DataFrame(metrics_data)
print("\nüìä Detailed Performance Metrics:")
print(metrics_df.round(3).to_string(index=False))

# Visualize metrics comparison
metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
dt_scores = [metrics_df.iloc[0][metric] for metric in metrics_to_plot]
rf_scores = [metrics_df.iloc[1][metric] for metric in metrics_to_plot]

x_pos = np.arange(len(metrics_to_plot))
axes[1,1].bar(x_pos - width/2, dt_scores, width, label='Decision Tree', alpha=0.7)
axes[1,1].bar(x_pos + width/2, rf_scores, width, label='Random Forest', alpha=0.7)
axes[1,1].set_xlabel('Metrics')
axes[1,1].set_ylabel('Score')
axes[1,1].set_title('Performance Metrics Comparison')
axes[1,1].set_xticks(x_pos)
axes[1,1].set_xticklabels(metrics_to_plot)
axes[1,1].legend()
axes[1,1].set_ylim(0, 1)

# Add value labels on bars
for i, (dt_score, rf_score) in enumerate(zip(dt_scores, rf_scores)):
    axes[1,1].text(i - width/2, dt_score + 0.01, f'{dt_score:.3f}', 
                   ha='center', va='bottom', fontsize=9)
    axes[1,1].text(i + width/2, rf_score + 0.01, f'{rf_score:.3f}', 
                   ha='center', va='bottom', fontsize=9)

# Feature importance ranking
importance_ranking = pd.DataFrame({
    'Feature': features_student,
    'DT_Importance': dt_importance,
    'RF_Importance': rf_importance
}).sort_values('RF_Importance', ascending=False)

axes[1,2].barh(range(len(features_student)), importance_ranking['RF_Importance'], 
               alpha=0.7, color='green', label='Random Forest')
axes[1,2].barh(range(len(features_student)), importance_ranking['DT_Importance'], 
               alpha=0.7, color='blue', label='Decision Tree')
axes[1,2].set_xlabel('Importance')
axes[1,2].set_ylabel('Features')
axes[1,2].set_title('Feature Importance Ranking')
axes[1,2].set_yticks(range(len(features_student)))
axes[1,2].set_yticklabels(importance_ranking['Feature'])
axes[1,2].legend()

plt.tight_layout()
plt.show()

print("\nüîç Key Insights:")
print("‚Ä¢ Most important factors for graduation success:")
for i, row in importance_ranking.head(3).iterrows():
    print(f"  {i+1}. {row['Feature']}: RF={row['RF_Importance']:.3f}, DT={row['DT_Importance']:.3f}")

# Test with new students
print("\nüéØ Prediction Examples:")
new_students = np.array([
    [3.8, 1400, 80, 0, 25, 8],   # High achiever
    [2.5, 950, 35, 1, 8, 15],    # At-risk student  
    [3.2, 1150, 55, 0, 15, 12]   # Average student
])

student_profiles = ['High Achiever', 'At-Risk Student', 'Average Student']

for i, (profile, student_data) in enumerate(zip(student_profiles, new_students)):
    dt_prob = best_dt.predict_proba([student_data])[0][1]
    rf_prob = best_rf.predict_proba([student_data])[0][1]
    
    print(f"\n{profile}:")
    print(f"  Profile: GPA={student_data[0]:.1f}, SAT={student_data[1]:.0f}, Income=${student_data[2]:.0f}k")
    print(f"  Study hrs={student_data[4]:.0f}, Social hrs={student_data[5]:.0f}, First-gen={'Yes' if student_data[3] else 'No'}")
    print(f"  Decision Tree prediction: {dt_prob:.1%} chance of graduation")
    print(f"  Random Forest prediction: {rf_prob:.1%} chance of graduation")

## Cross-validation for robust evaluation
print("\nüìà Cross-Validation Results (5-fold):")
dt_cv_scores = cross_val_score(best_dt, X_student, y_student, cv=5, scoring='accuracy')
rf_cv_scores = cross_val_score(best_rf, X_student, y_student, cv=5, scoring='accuracy')

print(f"Decision Tree: {dt_cv_scores.mean():.3f} ¬± {dt_cv_scores.std():.3f}")
print(f"Random Forest:  {rf_cv_scores.mean():.3f} ¬± {rf_cv_scores.std():.3f}")

plt.figure(figsize=(10, 6))
plt.boxplot([dt_cv_scores, rf_cv_scores], labels=['Decision Tree', 'Random Forest'])
plt.ylabel('Accuracy')
plt.title('Cross-Validation Performance Comparison')
plt.grid(True, alpha=0.3)
plt.show()

### ü§î Final Reflection: When to Use Each Algorithm

**Decision Trees - Best for:**
- Interpretability is crucial
- Small to medium datasets
- Mixed data types (numerical and categorical)
- Understanding decision logic
- Quick prototyping

**Random Forests - Best for:**
- Higher accuracy is priority
- Large datasets
- Robust predictions needed
- Feature importance analysis
- Handling overfitting

**KNN - Best for:**
- Simple, intuitive approach
- Non-linear relationships
- Local patterns matter
- Anomaly detection
- Recommendation systems

## Summary: Key Takeaways for Academic Applications

### üî¨ **Research Applications by Field:**

**Education:** Student success prediction, learning analytics, intervention targeting
**Healthcare:** Diagnosis support, treatment recommendations, risk assessment  
**Psychology:** Behavior prediction, survey analysis, clinical decision support
**Economics:** Market analysis, policy impact assessment, risk modeling
**Social Sciences:** Survey data analysis, demographic studies, social network analysis

### üìä **Algorithm Selection Guide:**

| Criterion | KNN | Decision Tree | Random Forest |
|-----------|-----|---------------|---------------|
| Interpretability | Medium | High | Low |
| Training Speed | Fast | Fast | Medium |
| Prediction Speed | Slow | Fast | Medium |
| Accuracy | Good | Good | Excellent |
| Overfitting Risk | Medium | High | Low |
| Memory Usage | High | Low | High |

### üéØ **Best Practices:**
1. **Always start with exploratory data analysis**
2. **Try multiple algorithms and compare**
3. **Use cross-validation for robust evaluation**
4. **Consider interpretability vs. accuracy tradeoffs**
5. **Validate on domain expert knowledge**
6. **Document assumptions and limitations**

### ‚ö†Ô∏è **Common Pitfalls:**
- Not scaling features for KNN
- Overfitting with deep decision trees
- Ignoring class imbalance
- Not validating on held-out data
- Over-interpreting feature importance

---

## üöÄ Next Steps for Your Research

1. **Identify a problem in your field** that could benefit from classification
2. **Collect or find relevant data** with clear outcome variables
3. **Start simple** - try the algorithms from this notebook
4. **Iterate and improve** based on domain knowledge
5. **Validate with colleagues** who understand the domain
6. **Consider ethical implications** of your model's decisions

**Remember:** Machine learning is a tool to augment human expertise, not replace it. The most successful applications combine algorithmic power with domain knowledge and ethical consideration.

---

*"The best machine learning projects are those that solve real problems and can be understood by the people who will use them."*

## Additional Resources for Continued Learning

### üìö **Recommended Reading:**
- "The Elements of Statistical Learning" by Hastie, Tibshirani, and Friedman
- "Pattern Recognition and Machine Learning" by Christopher Bishop
- "Applied Predictive Modeling" by Kuhn and Johnson

### üåê **Online Resources:**
- Scikit-learn documentation: https://scikit-learn.org/
- Kaggle Learn: https://www.kaggle.com/learn
- Coursera Machine Learning Course by Andrew Ng

### üõ†Ô∏è **Tools and Libraries:**
- **scikit-learn**: General-purpose ML library
- **pandas**: Data manipulation and analysis
- **matplotlib/seaborn**: Data visualization
- **jupyter**: Interactive computing environment

### ü§ù **Community:**
- Stack Overflow: For technical questions
- Reddit r/MachineLearning: For discussions and news
- Local ML meetups: For networking and learning

---

## Appendix: Code Templates for Your Projects

### Template 1: Basic Classification Pipeline

```python
# 1. Import libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 2. Load and explore data
df = pd.read_csv('your_data.csv')
print(df.head())
print(df.info())

# 3. Prepare features and target
X = df.drop('target_column', axis=1)
y = df['target_column']

# 4. Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 5. Scale features (for KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 6. Train models
knn = KNeighborsClassifier(n_neighbors=5)
dt = DecisionTreeClassifier(max_depth=5, random_state=42)
rf = RandomForestClassifier(n_estimators=100, random_state=42)

knn.fit(X_train_scaled, y_train)
dt.fit(X_train, y_train)
rf.fit(X_train, y_train)

# 7. Evaluate
knn_pred = knn.predict(X_test_scaled)
dt_pred = dt.predict(X_test)
rf_pred = rf.predict(X_test)

print(f"KNN Accuracy: {accuracy_score(y_test, knn_pred):.3f}")
print(f"Decision Tree Accuracy: {accuracy_score(y_test, dt_pred):.3f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.3f}")
```

### Template 2: Hyperparameter Tuning

```python
from sklearn.model_selection import GridSearchCV

# Define parameter grids
knn_params = {'n_neighbors': [3, 5, 7, 9, 11]}
dt_params = {'max_depth': [3, 5, 7, 10, None], 'min_samples_split': [2, 5, 10]}
rf_params = {'n_estimators': [50, 100, 200], 'max_depth': [5, 10, None]}

# Grid search
knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5)
dt_grid = GridSearchCV(DecisionTreeClassifier(random_state=42), dt_params, cv=5)
rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=5)

# Fit and find best parameters
knn_grid.fit(X_train_scaled, y_train)
dt_grid.fit(X_train, y_train)
rf_grid.fit(X_train, y_train)

print(f"Best KNN params: {knn_grid.best_params_}")
print(f"Best DT params: {dt_grid.best_params_}")
print(f"Best RF params: {rf_grid.best_params_}")
```

---

**Thank you for completing this machine learning journey! Remember: The goal is not to become a machine learning expert overnight, but to understand these tools well enough to apply them thoughtfully in your research domain. Start small, iterate often, and always validate your results with domain expertise.**

**Happy modeling! üöÄ**{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Academics: KNN and Decision Trees\n",
    "## A Practical Introduction for Professors Across Disciplines\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the intuition behind K-Nearest Neighbors (KNN) classification\n",
    "- Learn when and how to apply Decision Trees and Random Forests\n",
    "- Gain hands-on experience with real-world applications\n",
    "- Develop critical thinking about algorithm selection and limitations\n",
    "\n",
    "**Prerequisites:** Basic Python knowledge, introductory statistics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, load_iris, load_wine, load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hour 1: K-Nearest Neighbors (KNN)\n",
    "\n",
    "## Discovery Phase: Understanding the Intuition\n",
    "\n",
    "### The Academic Collaboration Analogy\n",
    "\n",
    "Imagine you're at an interdisciplinary conference and need to find collaborators for a new research project. How would you naturally approach this?\n",
    "\n",
    "1. **Look around your immediate vicinity** - Who's sitting near you?\n",
    "2. **Consider similarity** - Who shares your research interests, methodology, or field?\n",
    "3. **Make a decision** - Based on your closest \"neighbors\" in the research space\n",
    "\n",
    "This is exactly how K-Nearest Neighbors works! Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple academic collaboration scenario\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data representing researchers\n",
    "# Features: Years of experience, Number of publications\n",
    "stem_researchers = np.random.multivariate_normal([15, 50], [[25, 10], [10, 100]], 30)\n",
    "humanities_researchers = np.random.multivariate_normal([12, 25], [[20, 5], [5, 50]], 30)\n",
    "\n",
    "# Combine data\n",
    "X_research = np.vstack([stem_researchers, humanities_researchers])\n",
    "y_research = np.array(['STEM'] * 30 + ['Humanities'] * 30)\n",
    "\n",
    "# Create visualization\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: The research landscape\n",
    "colors = ['blue' if label == 'STEM' else 'red' for label in y_research]\n",
    "ax[0].scatter(X_research[:, 0], X_research[:, 1], c=colors, alpha=0.6, s=60)\n",
    "ax[0].set_xlabel('Years of Experience')\n",
    "ax[0].set_ylabel('Number of Publications')\n",
    "ax[0].set_title('Academic Research Landscape')\n",
    "ax[0].legend(['STEM', 'Humanities'])\n",
    "\n",
    "# Add a new researcher (unknown field)\n",
    "new_researcher = [14, 35]\n",
    "ax[0].scatter(new_researcher[0], new_researcher[1], c='green', s=200, marker='*', \n",
    "             edgecolors='black', linewidth=2, label='New Researcher')\n",
    "ax[0].legend(['STEM', 'Humanities', 'New Researcher'])\n",
    "\n",
    "# Plot 2: Find the nearest neighbors\n",
    "ax[1].scatter(X_research[:, 0], X_research[:, 1], c=colors, alpha=0.3, s=60)\n",
    "ax[1].scatter(new_researcher[0], new_researcher[1], c='green', s=200, marker='*', \n",
    "             edgecolors='black', linewidth=2)\n",
    "\n",
    "# Calculate distances and find k=5 nearest neighbors\n",
    "distances = np.sqrt(np.sum((X_research - new_researcher)**2, axis=1))\n",
    "nearest_indices = np.argsort(distances)[:5]\n",
    "\n",
    "# Highlight nearest neighbors\n",
    "for i in nearest_indices:\n",
    "    ax[1].scatter(X_research[i, 0], X_research[i, 1], \n",
    "                 c='gold', s=100, alpha=0.8, edgecolors='black')\n",
    "    # Draw lines to show connections\n",
    "    ax[1].plot([new_researcher[0], X_research[i, 0]], \n",
    "              [new_researcher[1], X_research[i, 1]], \n",
    "              'k--', alpha=0.5, linewidth=1)\n",
    "\n",
    "ax[1].set_xlabel('Years of Experience')\n",
    "ax[1].set_ylabel('Number of Publications')\n",
    "ax[1].set_title('KNN: Finding the 5 Nearest Neighbors')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show the classification result\n",
    "neighbor_labels = y_research[nearest_indices]\n",
    "print(f\"\\nThe 5 nearest neighbors are: {neighbor_labels}\")\n",
    "print(f\"Prediction: The new researcher is likely from {max(set(neighbor_labels), key=list(neighbor_labels).count)} field\")\n",
    "print(f\"Confidence: {list(neighbor_labels).count(max(set(neighbor_labels), key=list(neighbor_labels).count))/5*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§î Reflection Questions:\n",
    "1. Why might this approach work well for classification?\n",
    "2. What could go wrong with this method?\n",
    "3. How might the choice of k (number of neighbors) affect the prediction?\n",
    "\n",
    "---\n",
    "\n",
    "## Guided Practice: Building Your First KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's work with a real dataset: Wine classification\n",
    "# This dataset contains chemical analyses of wines from different cultivars\n",
    "wine_data = load_wine()\n",
    "X_wine = wine_data.data[:, :2]  # Use only first 2 features for visualization\n",
    "y_wine = wine_data.target\n",
    "feature_names = wine_data.feature_names[:2]\n",
    "target_names = wine_data.target_names\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Features: {feature_names}\")\n",
    "print(f\"Wine types: {target_names}\")\n",
    "print(f\"Dataset shape: {X_wine.shape}\")\n",
    "\n",
    "# Visualize the wine data\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, wine_type in enumerate(target_names):\n",
    "    mask = y_wine == i\n",
    "    plt.scatter(X_wine[mask, 0], X_wine[mask, 1], \n",
    "               c=colors[i], label=wine_type, alpha=0.7, s=60)\n",
    "\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.title('Wine Dataset: Chemical Composition')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wine, y_wine, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (important for KNN!)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Let's see why scaling matters\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Before scaling\n",
    "for i, wine_type in enumerate(target_names):\n",
    "    mask = y_train == i\n",
    "    axes[0].scatter(X_train[mask, 0], X_train[mask, 1], \n",
    "                   c=colors[i], label=wine_type, alpha=0.7)\n",
    "axes[0].set_title('Before Scaling')\n",
    "axes[0].set_xlabel(feature_names[0])\n",
    "axes[0].set_ylabel(feature_names[1])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# After scaling\n",
    "for i, wine_type in enumerate(target_names):\n",
    "    mask = y_train == i\n",
    "    axes[1].scatter(X_train_scaled[mask, 0], X_train_scaled[mask, 1], \n",
    "                   c=colors[i], label=wine_type, alpha=0.7)\n",
    "axes[1].set_title('After Scaling')\n",
    "axes[1].set_xlabel(f'{feature_names[0]} (scaled)')\n",
    "axes[1].set_ylabel(f'{feature_names[1]} (scaled)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different values of k\n",
    "k_values = [1, 3, 5, 7, 9, 15, 21]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"k={k:2d}: Accuracy = {accuracy:.3f}\")\n",
    "\n",
    "# Plot accuracy vs k\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values, accuracies, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('KNN Performance vs Number of Neighbors')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values)\n",
    "for i, (k, acc) in enumerate(zip(k_values, accuracies)):\n",
    "    plt.annotate(f'{acc:.3f}', (k, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center')\n",
    "plt.show()\n",
    "\n",
    "best_k = k_values[np.argmax(accuracies)]\n",
    "print(f\"\\nüéØ Best k value: {best_k} with accuracy: {max(accuracies):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundaries for different k values\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "k_showcase = [1, 3, 5, 9, 15, 21]\n",
    "\n",
    "for idx, k in enumerate(k_showcase):\n",
    "    # Train KNN with specific k\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Create decision boundary\n",
    "    h = 0.02\n",
    "    x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                        np.arange(y_min, y_max, h))\n",
    "    \n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot decision boundary\n",
    "    axes[idx].contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.RdYlBu)\n",
    "    \n",
    "    # Plot training points\n",
    "    for i, wine_type in enumerate(target_names):\n",
    "        mask = y_train == i\n",
    "        axes[idx].scatter(X_train_scaled[mask, 0], X_train_scaled[mask, 1], \n",
    "                         c=colors[i], label=wine_type, alpha=0.8, s=50)\n",
    "    \n",
    "    axes[idx].set_title(f'KNN Decision Boundary (k={k})')\n",
    "    axes[idx].set_xlabel('Feature 1 (scaled)')\n",
    "    axes[idx].set_ylabel('Feature 2 (scaled)')\n",
    "    if idx == 0:\n",
    "        axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Observations:\")\n",
    "print(\"‚Ä¢ k=1: Very complex boundary, might overfit\")\n",
    "print(\"‚Ä¢ k=3-5: Good balance, smooth but responsive\")\n",
    "print(\"‚Ä¢ k>15: Very smooth, might underfit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best k\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn.fit(X_train_scaled, y_train)\n",
    "y_pred_final = final_knn.predict(X_test_scaled)\n",
    "\n",
    "# Detailed performance metrics\n",
    "print(\"üéØ Final KNN Performance Report\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Best k value: {best_k}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_final):.3f}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_final, target_names=target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_final)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(f'Confusion Matrix (k={best_k})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Academic Department Classification\n",
    "\n",
    "**Scenario:** You're a university administrator trying to predict which department a faculty member belongs to based on their research profile.\n",
    "\n",
    "**Your Task:** Build a KNN classifier to predict department membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic faculty data\n",
    "np.random.seed(123)\n",
    "\n",
    "# Create faculty profiles: [Grant funding ($000s), Publications per year, Students supervised]\n",
    "# Engineering faculty\n",
    "engineering = np.random.multivariate_normal([200, 8, 4], [[5000, 100, 20], [100, 16, 8], [20, 8, 4]], 40)\n",
    "\n",
    "# Social Sciences faculty  \n",
    "social_sci = np.random.multivariate_normal([80, 4, 6], [[1600, 40, 10], [40, 4, 2], [10, 2, 4]], 40)\n",
    "\n",
    "# Natural Sciences faculty\n",
    "natural_sci = np.random.multivariate_normal([150, 12, 3], [[3600, 144, 18], [144, 36, 6], [18, 6, 2]], 40)\n",
    "\n",
    "# Combine data\n",
    "X_faculty = np.vstack([engineering, social_sci, natural_sci])\n",
    "y_faculty = np.array(['Engineering'] * 40 + ['Social Sciences'] * 40 + ['Natural Sciences'] * 40)\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "faculty_df = pd.DataFrame(X_faculty, columns=['Grant_Funding_K', 'Publications_Per_Year', 'Students_Supervised'])\n",
    "faculty_df['Department'] = y_faculty\n",
    "\n",
    "print(\"üéì Faculty Dataset Overview:\")\n",
    "print(faculty_df.groupby('Department').describe().round(2))\n",
    "\n",
    "# Visualize the data\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 3D scatter plot\n",
    "ax = fig.add_subplot(221, projection='3d')\n",
    "colors_dept = {'Engineering': 'red', 'Social Sciences': 'blue', 'Natural Sciences': 'green'}\n",
    "for dept in faculty_df['Department'].unique():\n",
    "    mask = faculty_df['Department'] == dept\n",
    "    ax.scatter(faculty_df[mask]['Grant_Funding_K'], \n",
    "              faculty_df[mask]['Publications_Per_Year'],\n",
    "              faculty_df[mask]['Students_Supervised'],\n",
    "              c=colors_dept[dept], label=dept, alpha=0.7, s=50)\n",
    "\n",
    "ax.set_xlabel('Grant Funding ($000s)')\n",
    "ax.set_ylabel('Publications/Year')\n",
    "ax.set_zlabel('Students Supervised')\n",
    "ax.set_title('Faculty Profiles by Department')\n",
    "ax.legend()\n",
    "\n",
    "# 2D projections\n",
    "projections = [(0, 1, '2D: Funding vs Publications'), \n",
    "               (0, 2, '2D: Funding vs Students'), \n",
    "               (1, 2, '2D: Publications vs Students')]\n",
    "\n",
    "for i, (x_idx, y_idx, title) in enumerate(projections):\n",
    "    ax = fig.add_subplot(2, 2, i+2)\n",
    "    for dept in faculty_df['Department'].unique():\n",
    "        mask = faculty_df['Department'] == dept\n",
    "        ax.scatter(X_faculty[faculty_df['Department'] == dept, x_idx], \n",
    "                  X_faculty[faculty_df['Department'] == dept, y_idx],\n",
    "                  c=colors_dept[dept], label=dept, alpha=0.7, s=50)\n",
    "    \n",
    "    ax.set_xlabel(faculty_df.columns[x_idx])\n",
    "    ax.set_ylabel(faculty_df.columns[y_idx])\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR TURN: Complete the KNN analysis\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "X_exercise = X_faculty\n",
    "y_exercise = y_faculty\n",
    "\n",
    "# TODO: Split into train/test sets (70/30 split)\n",
    "# YOUR CODE HERE:\n",
    "X_train_ex, X_test_ex, y_train_ex, y_test_ex = train_test_split(\n",
    "    X_exercise, y_exercise, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# TODO: Scale the features\n",
    "# YOUR CODE HERE:\n",
    "scaler_ex = StandardScaler()\n",
    "X_train_scaled_ex = scaler_ex.fit_transform(X_train_ex)\n",
    "X_test_scaled_ex = scaler_ex.transform(X_test_ex)\n",
    "\n",
    "# TODO: Test different k values and find the best one\n",
    "k_values_ex = [1, 3, 5, 7, 9, 11, 15, 19]\n",
    "accuracies_ex = []\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "for k in k_values_ex:\n",
    "    knn_ex = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn_ex.fit(X_train_scaled_ex, y_train_ex)\n",
    "    y_pred_ex = knn_ex.predict(X_test_scaled_ex)\n",
    "    acc = accuracy_score(y_test_ex, y_pred_ex)\n",
    "    accuracies_ex.append(acc)\n",
    "    print(f\"k={k:2d}: Accuracy = {acc:.3f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_values_ex, accuracies_ex, 'ro-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Faculty Department Classification: KNN Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(k_values_ex)\n",
    "for k, acc in zip(k_values_ex, accuracies_ex):\n",
    "    plt.annotate(f'{acc:.3f}', (k, acc), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center')\n",
    "plt.show()\n",
    "\n",
    "best_k_ex = k_values_ex[np.argmax(accuracies_ex)]\n",
    "print(f\"\\nüèÜ Best k: {best_k_ex} with accuracy: {max(accuracies_ex):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final model evaluation\n",
    "final_knn_ex = KNeighborsClassifier(n_neighbors=best_k_ex)\n",
    "final_knn_ex.fit(X_train_scaled_ex, y_train_ex)\n",
    "y_pred_final_ex = final_knn_ex.predict(X_test_scaled_ex)\n",
    "\n",
    "print(\"üéØ Faculty Department Classification Results\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best k: {best_k_ex}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test_ex, y_pred_final_ex):.3f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_ex, y_pred_final_ex))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_ex = confusion_matrix(y_test_ex, y_pred_final_ex)\n",
    "disp_ex = ConfusionMatrixDisplay(confusion_matrix=cm_ex, \n",
    "                                display_labels=['Engineering', 'Natural Sciences', 'Social Sciences'])\n",
    "disp_ex.plot(cmap='Oranges')\n",
    "plt.title(f'Department Classification Confusion Matrix (k={best_k_ex})')\n",
    "plt.show()\n",
    "\n",
    "# Test with a new faculty member\n",
    "new_faculty = np.array([[120, 6, 5]])  # Funding, Publications, Students\n",
    "new_faculty_scaled = scaler_ex.transform(new_faculty)\n",
    "prediction = final_knn_ex.predict(new_faculty_scaled)\n",
    "probabilities = final_knn_ex.predict_proba(new_faculty_scaled)\n",
    "\n",
    "print(f\"\\nüîÆ New Faculty Prediction:\")\n",
    "print(f\"Profile: ${new_faculty[0,0]}k funding, {new_faculty[0,1]} pubs/year, {new_faculty[0,2]} students\")\n",
    "print(f\"Predicted Department: {prediction[0]}\")\n",
    "print(f"Confidence: {max(probabilities[0]):.3f}")

### ü§î Reflection: KNN Strengths and Limitations

**Strengths:**
- Simple and intuitive
- No assumptions about data distribution
- Works well with sufficient data
- Can capture complex decision boundaries

**Limitations:**
- Sensitive to irrelevant features
- Computationally expensive for large datasets
- Sensitive to local structure of data
- Requires careful choice of k

---

# Hour 2: Decision Trees and Random Forests

## Discovery Phase: The Academic Decision Process

### The Research Paper Review Analogy

Imagine you're reviewing a research paper for a journal. How do you make decisions?

1. **Start with broad criteria** - Is the methodology sound?
2. **Ask specific questions** - If methodology is good, is the sample size adequate?
3. **Continue narrowing down** - If sample size is good, are the results significant?
4. **Make final decision** - Accept, Reject, or Revise

This hierarchical decision-making process is exactly how decision trees work!

## Understanding Decision Trees with Academic Examples

# Let's create a simple academic decision scenario
# Predicting if a research proposal will be funded

# Generate synthetic research proposal data
np.random.seed(42)
n_proposals = 1000

# Features: Budget (in $10k), PI Experience (years), Institution Ranking (1-100), Innovation Score (1-10)
budget = np.random.exponential(5, n_proposals) + 1  # $10k - $100k+
pi_experience = np.random.gamma(2, 5, n_proposals)  # 0-20+ years
institution_rank = np.random.uniform(1, 100, n_proposals)  # 1-100
innovation_score = np.random.normal(5, 2, n_proposals)  # 1-10 scale
innovation_score = np.clip(innovation_score, 1, 10)

# Create funding decisions based on realistic criteria
funding_probability = (
    0.3 * (1 / (1 + np.exp(-(pi_experience - 5) / 2))) +  # Experience matters
    0.2 * (1 / (1 + np.exp(-(institution_rank - 50) / 10))) +  # Better institutions help
    0.3 * (innovation_score / 10) +  # Innovation is key
    0.2 * (1 / (1 + np.exp((budget - 8) / 2))) +  # Lower budgets more likely
    np.random.normal(0, 0.1, n_proposals)  # Random noise
)

funded = (funding_probability > 0.6).astype(int)

# Create DataFrame
proposals_df = pd.DataFrame({
    'Budget_10k': budget,
    'PI_Experience': pi_experience,
    'Institution_Rank': institution_rank,
    'Innovation_Score': innovation_score,
    'Funded': funded
})

print("üî¨ Research Proposal Dataset:")
print(proposals_df.head(10))
print(f"\nFunding Rate: {funded.mean():.1%}")
print(f"Dataset Shape: {proposals_df.shape}")

# Visualize the relationships
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

features = ['Budget_10k', 'PI_Experience', 'Institution_Rank', 'Innovation_Score']
for i, feature in enumerate(features):
    ax = axes[i//2, i%2]
    
    # Separate funded and unfunded
    funded_data = proposals_df[proposals_df['Funded'] == 1][feature]
    unfunded_data = proposals_df[proposals_df['Funded'] == 0][feature]
    
    ax.hist(unfunded_data, alpha=0.6, label='Not Funded', bins=30, color='red')
    ax.hist(funded_data, alpha=0.6, label='Funded', bins=30, color='green')
    ax.set_xlabel(feature)
    ax.set_ylabel('Count')
    ax.set_title(f'Distribution of {feature}')
    ax.legend()
    ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# YOUR TURN: Build and compare models

# Prepare data
features_student = ['HS_GPA', 'SAT_Score', 'Family_Income_K', 'First_Generation', 
                   'Study_Hours_Week', 'Social_Activities_Week']
X_student = students_df[features_student].values
y_student = students_df['Graduates'].values

# TODO: Split the data
# YOUR CODE HERE:
X_train_student, X_test_student, y_train_student, y_test_student = train_test_split(
    X_student, y_student, test_size=0.25, random_state=42, stratify=y_student
)

print(f"Training set: {len(X_train_student)} students")
print(f"Test set: {len(X_test_student)} students")
print(f"Training graduation rate: {y_train_student.mean():.1%}")
print(f"Test graduation rate: {y_test_student.mean():.1%}")

# TODO: Build Decision Tree with different complexities
dt_complexities = [3, 5, 8, 10, 15, None]
dt_results = []

for max_depth in dt_complexities:
    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)
    dt.fit(X_train_student, y_train_student)
    
    train_acc = accuracy_score(y_train_student, dt.predict(X_train_student))
    test_acc = accuracy_score(y_test_student, dt.predict(X_test_student))
    
    dt_results.append({
        'max_depth': max_depth,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc,
        'overfitting': train_acc - test_acc
    })
    
    depth_str = str(max_depth) if max_depth else "Unlimited"
    print(f"DT Depth {depth_str:>9}: Train={train_acc:.3f}, Test={test_acc:.3f}, Gap={train_acc - test_acc:.3f}")

# TODO: Build Random Forest with different numbers of trees
rf_n_trees = [10, 25, 50, 100, 200]
rf_results = []

for n_trees in rf_n_trees:
    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)
    rf.fit(X_train_student, y_train_student)
    
    train_acc = accuracy_score(y_train_student, rf.predict(X_train_student))
    test_acc = accuracy_score(y_test_student, rf.predict(X_test_student))
    
    rf_results.append({
        'n_estimators': n_trees,
        'train_accuracy': train_acc,
        'test_accuracy': test_acc,
        'overfitting': train_acc - test_acc
    })
    
    print(f"RF Trees {n_trees:>3}: Train={train_acc:.3f}, Test={test_acc:.3f}, Gap={train_acc - test_acc:.3f}")

# Plot comparison
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Decision Tree Complexity
dt_df = pd.DataFrame(dt_results)
depths_for_plot = [str(d) if d is not None else "‚àû" for d in dt_df['max_depth']]

axes[0,0].plot(range(len(dt_df)), dt_df['train_accuracy'], 'bo-', label='Train')
axes[0,0].plot(range(len(dt_df)), dt_df['test_accuracy'], 'ro-', label='Test')
axes[0,0].set_xlabel('Tree Depth')
axes[0,0].set_ylabel('Accuracy')
axes[0,0].set_title('Decision Tree: Complexity vs Performance')
axes[0,0].set_xticks(range(len(dt_df)))
axes[0,0].set_xticklabels(depths_for_plot)
axes[0,0].legend()
axes[0,0].grid(True, alpha=0.3)

# Random Forest Trees
rf_df = pd.DataFrame(rf_results)
axes[0,1].plot(rf_df['n_estimators'], rf_df['train_accuracy'], 'go-', label='Train')
axes[0,1].plot(rf_df['n_estimators'], rf_df['test_accuracy'], 'mo-', label='Test')
axes[0,1].set_xlabel('Number of Trees')
axes[0,1].set_ylabel('Accuracy')
axes[0,1].set_title('Random Forest: Trees vs Performance')
axes[0,1].legend()
axes[0,1].grid(True, alpha=0.3)

# Overfitting comparison
axes[1,0].bar(range(len(dt_df)), dt_df['overfitting'], alpha=0.7, label='Decision Tree')
axes[1,0].set_xlabel('Tree Depth') 
axes[1,0].set_ylabel('Overfitting (Train - Test)')
axes[1,0].set_title('Decision Tree Overfitting')
axes[1,0].set_xticks(range(len(dt_df)))
axes[1,0].set_xticklabels(depths_for_plot)
axes[1,0].grid(True, alpha=0.3)

axes[1,1].bar(range(len(rf_df)), rf_df['overfitting'], alpha=0.7, color='green', label='Random Forest')
axes[1,1].set_xlabel('Number of Trees')
axes[1,1].set_ylabel('Overfitting (Train - Test)')
axes[1,1].set_title('Random Forest Overfitting')
axes[1,1].set_xticks(range(len(rf_df)))
axes[1,1].set_xticklabels(rf_df['n_estimators'])
axes[1,1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Now let's build our first decision tree
X_proposals = proposals_df[features].values
y_proposals = proposals_df['Funded'].values

# Split the data
X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(
    X_proposals, y_proposals, test_size=0.3, random_state=42
)

# Build a simple decision tree
dt_simple = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_simple.fit(X_train_dt, y_train_dt)

# Visualize the decision tree
plt.figure(figsize=(20, 12))
plot_tree(dt_simple, 
         feature_names=features,
         class_names=['Not Funded', 'Funded'],
         filled=True,
         rounded=True,
         fontsize=12)
plt.title('Research Funding Decision Tree', fontsize=16)
plt.show()

# Evaluate performance
y_pred_dt = dt_simple.predict(X_test_dt)
accuracy_dt = accuracy_score(y_test_dt, y_pred_dt)

print(f"\nüéØ Decision Tree Performance:")
print(f"Accuracy: {accuracy_dt:.3f}")
print("\nClassification Report:")
print(classification_report(y_test_dt, y_pred_dt, target_names=['Not Funded', 'Funded']))

## Guided Practice: Comparing Decision Tree Complexity

# Let's explore how tree depth affects performance
depths = [1, 2, 3, 5, 10, 15, None]  # None means no limit
train_accuracies = []
test_accuracies = []

for depth in depths:
    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)
    dt.fit(X_train_dt, y_train_dt)
    
    train_acc = accuracy_score(y_train_dt, dt.predict(X_train_dt))
    test_acc = accuracy_score(y_test_dt, dt.predict(X_test_dt))
    
    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)
    
    depth_str = str(depth) if depth is not None else "Unlimited"
    print(f"Depth {depth_str:>9}: Train={train_acc:.3f}, Test={test_acc:.3f}")

# Plot the bias-variance tradeoff
plt.figure(figsize=(12, 8))
x_pos = range(len(depths))
depth_labels = [str(d) if d is not None else "‚àû" for d in depths]

plt.subplot(2, 1, 1)
plt.plot(x_pos, train_accuracies, 'bo-', label='Training Accuracy', linewidth=2, markersize=8)
plt.plot(x_pos, test_accuracies, 'ro-', label='Testing Accuracy', linewidth=2, markersize=8)
plt.xlabel('Tree Depth')
plt.ylabel('Accuracy')
plt.title('Decision Tree: Training vs Testing Performance')
plt.xticks(x_pos, depth_labels)
plt.legend()
plt.grid(True, alpha=0.3)

# Show overfitting
plt.subplot(2, 1, 2)
overfitting = np.array(train_accuracies) - np.array(test_accuracies)
plt.bar(x_pos, overfitting, color=['green' if x < 0.05 else 'orange' if x < 0.1 else 'red' for x in overfitting])
plt.xlabel('Tree Depth')
plt.ylabel('Overfitting (Train - Test)')
plt.title('Overfitting Analysis')
plt.xticks(x_pos, depth_labels)
plt.axhline(y=0.05, color='orange', linestyle='--', label='Moderate Overfitting')
plt.axhline(y=0.1, color='red', linestyle='--', label='High Overfitting')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nüìä Key Insights:")
print("‚Ä¢ Shallow trees: High bias, low variance (underfitting)")
print("‚Ä¢ Deep trees: Low bias, high variance (overfitting)")
print("‚Ä¢ Sweet spot: Balance between bias and variance")

## Random Forests: Wisdom of the Academic Committee

### The Peer Review Analogy

Instead of relying on one reviewer, journals use multiple reviewers. Random Forest works similarly:
- Build many decision trees (reviewers)
- Each tree sees a random subset of data and features  
- Final decision is based on majority vote
- More robust than single decision

# Let's see this in action with our funding data
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_dt, y_train_dt)

# Compare single tree vs random forest
dt_best = DecisionTreeClassifier(max_depth=5, random_state=42)  # Best depth from above
dt_best.fit(X_train_dt, y_train_dt)

# Predictions
dt_pred = dt_best.predict(X_test_dt)
rf_pred = rf_model.predict(X_test_dt)

# Performance comparison
print("üå≤ Single Decision Tree vs üå≥ Random Forest")
print("=" * 50)
print(f"Decision Tree Accuracy: {accuracy_score(y_test_dt, dt_pred):.3f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test_dt, rf_pred):.3f}")

# Detailed comparison
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# Confusion matrices
cm_dt = confusion_matrix(y_test_dt, dt_pred)
cm_rf = confusion_matrix(y_test_dt, rf_pred)

disp_dt = ConfusionMatrixDisplay(confusion_matrix=cm_dt, display_labels=['Not Funded', 'Funded'])
disp_dt.plot(ax=axes[0], cmap='Blues')
axes[0].set_title('Decision Tree\nConfusion Matrix')

disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=['Not Funded', 'Funded'])
disp_rf.plot(ax=axes[1], cmap='Greens')
axes[1].set_title('Random Forest\nConfusion Matrix')

# Feature importance comparison
feature_importance_dt = dt_best.feature_importances_
feature_importance_rf = rf_model.feature_importances_

x_pos = np.arange(len(features))
width = 0.35

axes[2].bar(x_pos - width/2, feature_importance_dt, width, label='Decision Tree', alpha=0.7)
axes[2].bar(x_pos + width/2, feature_importance_rf, width, label='Random Forest', alpha=0.7)
axes[2].set_xlabel('Features')
axes[2].set_ylabel('Importance')
axes[2].set_title('Feature Importance Comparison')
axes[2].set_xticks(x_pos)
axes[2].set_xticklabels(features, rotation=45)
axes[2].legend()
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

# Show individual tree predictions (first 5 trees)
print("\nüîç Individual Tree Predictions (Sample):")
sample_idx = 42
sample_data = X_test_dt[sample_idx:sample_idx+1]
actual_label = y_test_dt[sample_idx]

tree_predictions = []
for i in range(min(10, rf_model.n_estimators)):
    tree_pred = rf_model.estimators_[i].predict(sample_data)[0]
    tree_predictions.append(tree_pred)

print(f"Sample prediction for test case {sample_idx}:")
print(f"Actual: {'Funded' if actual_label else 'Not Funded'}")
print(f"Individual trees: {tree_predictions}")
print(f"Majority vote: {'Funded' if sum(tree_predictions) > len(tree_predictions)/2 else 'Not Funded'}")
print(f"RF Prediction: {'Funded' if rf_model.predict(sample_data)[0] else 'Not Funded'}")

## Exercise 2: Student Success Prediction

**Scenario:** You're designing an early warning system to identify students at risk of not completing their degree.

**Your Task:** Build both Decision Tree and Random Forest models to predict student success.

# Generate synthetic student data
np.random.seed(456)
n_students = 2000

# Features that might affect graduation success
high_school_gpa = np.random.normal(3.2, 0.6, n_students)
high_school_gpa = np.clip(high_school_gpa, 1.0, 4.0)

sat_score = np.random.normal(1200, 200, n_students)
sat_score = np.clip(sat_score, 800, 1600)

family_income = np.random.lognormal(10.5, 0.8, n_students)  # In thousands
family_income = np.clip(family_income, 20, 200)

first_generation = np.random.choice([0, 1], n_students, p=[0.7, 0.3])

hours_studying = np.random.gamma(2, 8, n_students)
hours_studying = np.clip(hours_studying, 2, 50)

social_activities = np.random.normal(10, 5, n_students)
social_activities = np.clip(social_activities, 0, 25)

# Create success probability based on realistic factors
success_prob = (
    0.25 * (high_school_gpa / 4.0) +
    0.20 * ((sat_score - 800) / 800) +
    0.15 * (np.log(family_income) - np.log(20)) / (np.log(200) - np.log(20)) +
    0.15 * (hours_studying / 50) +
    0.10 * (social_activities / 25) +
    -0.05 * first_generation +  # First gen students face more challenges
    0.10 * np.random.normal(0, 1, n_students)  # Random factors
)

graduates = (success_prob > 0.5).astype(int)

# Create DataFrame
students_df = pd.DataFrame({
    'HS_GPA': high_school_gpa,
    'SAT_Score': sat_score,
    'Family_Income_K': family_income,
    'First_Generation': first_generation,
    'Study_Hours_Week': hours_studying,
    'Social_Activities_Week': social_activities,
    'Graduates': graduates
})

print("üéì Student Success Dataset:")
print(students_df.head())
print(f"\nGraduation Rate: {graduates.mean():.1%}")
print(f"Dataset Shape: {students_df.shape}")

# Exploratory analysis
print("\nüìà Success Rates by Group:")
print(f"First Generation: {students_df[students_df['First_Generation']==1]['Graduates'].mean():.1%}")
print(f"Non-First Generation: {students_df[students_df['First_Generation']==0]['Graduates'].mean():.1%}")

# Correlation analysis
plt.figure(figsize=(15, 10))

# Correlation heatmap
plt.subplot(2, 3, 1)
corr_matrix = students_df.corr()
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
            square=True, fmt='.2f')
plt.title('Feature Correlations')

# Distribution plots
features_to_plot = ['HS_GPA', 'SAT_Score', 'Family_Income_K', 'Study_Hours_Week', 'Social_Activities_Week']
for i, feature in enumerate(features_to_plot):
    plt.subplot(2, 3, i+2)
    
    graduates_data = students_df[students_df['Graduates'] == 1][feature]
    non_graduates_data = students_df[students_df['Graduates'] == 0][feature]
    
    plt.hist(non_graduates_data, alpha=0.6, label='Did Not Graduate', bins=25, color='red')
    plt.hist(graduates_data, alpha=0.6, label='Graduated', bins=25, color='blue')
    plt.xlabel(feature)
    plt.ylabel('Count')
    plt.title(f'{feature} Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
